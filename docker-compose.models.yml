models:
  coder:
    model: ai/qwen3-coder:latest
    context_size: 16384
    runtime_flags:
      - --flash-attn
      - "on"
      - --threads # Number of threads to use during generation
      - "8"
      - --mlock
      - --verbose # Set verbosity level to infinity
      - --verbose-prompt # Print a verbose prompt before generation
      - --log-prefix # Enable prefix in log messages
      - --log-timestamps # Enable timestamps in log messages
      - --log-colors # Enable colored log messages
      - "on"
      - --reasoning-format
      - deepseek
      - --context-shift
  general:
    model: ai/llama3.1:8B-F16
    context_size: 8192
    runtime_flags:
      - --flash-attn
      - "on"
      - --threads # Number of threads to use during generation
      - "8"
      - --mlock
      - --verbose # Set verbosity level to infinity
      - --verbose-prompt # Print a verbose prompt before generation
      - --log-prefix # Enable prefix in log messages
      - --log-timestamps # Enable timestamps in log messages
      - --log-colors # Enable colored log messages
      - "on"
      - --reasoning-format
      - deepseek
      - --context-shift

services:
  # api:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile
  #     args:
  #       APP_NAME: api
  #   environment:
  #     NODE_ENV: development
  #     EDDIE_CLI_CONFIG: /app/agent_work/code-assistant/eddie.config.yaml
  #   models:
  #     - llm

  # web:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile
  #     args:
  #       APP_NAME: web
  #   ports:
  #     - 5173:5173
  #   environment:
  #     NODE_ENV: development
  #   models:
  #     - llm

  app:
    image: node:22
    models:
      - general
      - coder
